{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a544d8d",
   "metadata": {},
   "source": [
    "## Few-Shot-Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db17338",
   "metadata": {},
   "source": [
    "### Divisão dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b8e149",
   "metadata": {},
   "source": [
    "Cria um dicionário dos dados (class map): {label_classe: [lista de  imagens dessa classe]} e divide os dados entre treino e teste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5d2955",
   "metadata": {},
   "source": [
    "**Estrutura do Omniglot**\n",
    "\n",
    "\n",
    "omniglot-py/ <br>\n",
    "└── images_background/  (ou images_evaluation/)<br>\n",
    "    ├── Alphabet_of_the_Magi/<br>\n",
    "    │   ├── character01/<br>\n",
    "    │   │   ├── 0709_01.png<br>\n",
    "    │   │   ├── 0709_02.png<br>\n",
    "    │   │   └── ... (20 imagens no total)<br>\n",
    "    │   └── character02/<br>\n",
    "    │       └── ...<br>\n",
    "    ├── Angelic/<br>\n",
    "    │   └── ...<br>\n",
    "    └── ... (mais 30 alfabetos)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd910afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "import torchvision as tv\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "omniglot_train = tv.datasets.Omniglot(\n",
    "    root=\"../data\",\n",
    "    background=True, #Conjunto de treino\n",
    "    download=True,\n",
    "    transform=transform \n",
    ")\n",
    "\n",
    "\n",
    "omniglot_test = tv.datasets.Omniglot(\n",
    "    root=\"../data\",\n",
    "    background=False, #Conjunto de teste\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71756d4c",
   "metadata": {},
   "source": [
    "O Omniglot é dividido em background e evaluation. O conjunto background contém os alfabetos para treinar o seu modelo a \"aprender a aprender\". O conjunto evaluation contém alfabetos completamente novos, que o modelo nunca viu, e é usado para testar se ele consegue generalizar para novas classes com poucos exemplos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3d70bd",
   "metadata": {},
   "source": [
    "### Episódios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6691654f",
   "metadata": {},
   "source": [
    "Em vez de treinar o modelo para ser um especialista em um conjunto fixo de classes, nós o treinamos para ser um generalista em aprender.\n",
    "\n",
    "Seria dar ao modelo uma série de \"mini testes surpresa\" (os episódios). Cada teste é uma pequena tarefa de classificação."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266417ca",
   "metadata": {},
   "source": [
    "**Anatomia de um Episódio**\n",
    "\n",
    "Em um cenário N-way K-shot (onde N = 5 e K = 1):\n",
    "\n",
    "1. Amostragem: Primeiro, escolhemos aleatoriamente N classes do nosso enorme conjunto de dados de treinamento. \n",
    "2. Support Set: Para cada uma dessas N classes, damos ao modelo K exemplos. Este é o \"material de estudo\" para este teste específico. O modelo deve olhar para essas \"N times K\" imagens e aprender a distinguir as N classes.\n",
    "3. Query Set: Em seguida, pegamos outras imagens (que não estavam no material de estudo) daquelas mesmas N classes. Estas são as \"perguntas do teste\".\n",
    "4. Avaliação e Aprendizado: O modelo tenta classificar as imagens do Query Set. Calculamos o quão bem ele se saiu (loss). Usamos essa nota para ajustar os pesos do modelo através do backpropagation.\n",
    "\n",
    "\n",
    "Ao repetir esse processo milhares de vezes, com milhares de combinações diferentes de classes, o modelo não está memorizando \"o que é o caractere A do alfabeto Grego\". Em vez disso, ele está aprendendo uma estratégia para, dado qualquer conjunto de N novas classes com K exemplos cada, descobrir as características que as diferenciam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85edbc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_episode(classes, n_way, k_shot, k_query):\n",
    "    episode_classes_idx = random.sample(list(classes.keys()), n_way)\n",
    "    \n",
    "    support_set = []\n",
    "    support_labels = []\n",
    "    query_set = []\n",
    "    query_labels = []\n",
    "    \n",
    "    \n",
    "    #* Montar o Support e o Query set para cada classe selecionada\n",
    "    for i, class_idx in enumerate(episode_classes_idx):\n",
    "        images_of_class = classes[class_idx] \n",
    "        \n",
    "        #* Sortear imagens dessa classe\n",
    "        selected_images = random.sample(images_of_class, k_shot + k_query)\n",
    "        \n",
    "        #* Dividir as imagens sorteadas nos dois conjuntos\n",
    "        support_images = selected_images[:k_shot]\n",
    "        query_images = selected_images[k_shot:]\n",
    "        \n",
    "        support_set.extend(support_images)\n",
    "        query_set.extend(query_images)\n",
    "        \n",
    "        #* Criar as labels RELATIVAS ao episódio\n",
    "        support_labels.extend([i] * k_shot)\n",
    "        query_labels.extend([i] * k_query)\n",
    "\n",
    "        #* Embaralhar os conjuntos\n",
    "        s_indices = np.random.permutation(len(support_set))\n",
    "        q_indices = np.random.permutation(len(query_set))\n",
    "        \n",
    "        #* Reordenar usando os índices embaralhados\n",
    "        support_set = torch.stack(support_set)[s_indices]\n",
    "        support_labels = torch.tensor(support_labels)[s_indices]\n",
    "        query_set = torch.stack(query_set)[q_indices]\n",
    "        query_labels = torch(query_labels)[q_indices]\n",
    "        \n",
    "        #*Retorna os 4 tensores para serem usados pelo modelo\n",
    "        return support_set, support_labels, query_set, query_labels\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d28455",
   "metadata": {},
   "source": [
    "### Metric Based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee50ba6",
   "metadata": {},
   "source": [
    "A ideia central é: aprender um espaço de características (embedding space) onde imagens da mesma classe fiquem muito próximas e imagens de classes diferentes fiquem distantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74d2030",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    \n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
