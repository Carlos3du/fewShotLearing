{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a544d8d",
   "metadata": {},
   "source": [
    "## Few-Shot-Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db17338",
   "metadata": {},
   "source": [
    "### Divisão dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b8e149",
   "metadata": {},
   "source": [
    "Cria um dicionário dos dados (class map): {label_classe: [lista de  imagens dessa classe]} e divide os dados entre treino e teste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5d2955",
   "metadata": {},
   "source": [
    "**Estrutura do Omniglot**\n",
    "\n",
    "\n",
    "omniglot-py/ <br>\n",
    "└── images_background/  (ou images_evaluation/)<br>\n",
    "    ├── Alphabet_of_the_Magi/<br>\n",
    "    │   ├── character01/<br>\n",
    "    │   │   ├── 0709_01.png<br>\n",
    "    │   │   ├── 0709_02.png<br>\n",
    "    │   │   └── ... (20 imagens no total)<br>\n",
    "    │   └── character02/<br>\n",
    "    │       └── ...<br>\n",
    "    ├── Angelic/<br>\n",
    "    │   └── ...<br>\n",
    "    └── ... (mais 30 alfabetos)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55723d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision as tv\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71756d4c",
   "metadata": {},
   "source": [
    "O Omniglot é dividido em background e evaluation. O conjunto background contém os alfabetos para treinar o seu modelo a \"aprender a aprender\". O conjunto evaluation contém alfabetos completamente novos, que o modelo nunca viu, e é usado para testar se ele consegue generalizar para novas classes com poucos exemplos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3d70bd",
   "metadata": {},
   "source": [
    "### Episódios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6691654f",
   "metadata": {},
   "source": [
    "- Em vez de treinar o modelo para ser um especialista em um conjunto fixo de classes, nós o treinamos para ser um generalista em aprender.\n",
    "- Seria dar ao modelo uma série de \"mini testes surpresa\" (os episódios). Cada teste é uma pequena tarefa de classificação."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266417ca",
   "metadata": {},
   "source": [
    "**Anatomia de um Episódio**\n",
    "\n",
    "Em um cenário N-way K-shot (onde N = 5 e K = 1):\n",
    "\n",
    "1. **Amostragem:** Primeiro, escolhemos aleatoriamente N classes do nosso enorme conjunto de dados de treinamento. \n",
    "2. **Support Set:** Para cada uma dessas N classes, damos ao modelo K exemplos. Este é o \"material de estudo\" para este teste específico. O modelo deve olhar para essas \"N times K\" imagens e aprender a distinguir as N classes.\n",
    "3. **Query Set:** Em seguida, pegamos outras imagens (que não estavam no material de estudo) daquelas mesmas N classes. Estas são as \"perguntas do teste\".\n",
    "4. **Avaliação e Aprendizado:** O modelo tenta classificar as imagens do Query Set. Calculamos o quão bem ele se saiu (loss). Usamos essa nota para ajustar os pesos do modelo através do backpropagation.\n",
    "\n",
    "\n",
    "Ao repetir esse processo milhares de vezes, com milhares de combinações diferentes de classes, o modelo não está memorizando \"o que é o caractere A do alfabeto Grego\". Em vez disso, ele está aprendendo uma estratégia para, dado qualquer conjunto de N novas classes com K exemplos cada, descobrir as características que as diferenciam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85edbc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_episode(classes, n_way, k_shot, k_query):\n",
    "    episode_classes_idx = random.sample(list(classes.keys()), n_way)\n",
    "    \n",
    "    support_set = []\n",
    "    support_labels = []\n",
    "    query_set = []\n",
    "    query_labels = []\n",
    "    \n",
    "    \n",
    "    #* Montar o Support e o Query set para cada classe selecionada\n",
    "    for i, class_idx in enumerate(episode_classes_idx):\n",
    "        images_of_class = classes[class_idx] \n",
    "        \n",
    "        #* Sortear imagens dessa classe\n",
    "        selected_images = random.sample(images_of_class, k_shot + k_query)\n",
    "        \n",
    "        #* Dividir as imagens sorteadas nos dois conjuntos\n",
    "        support_images = selected_images[:k_shot]\n",
    "        query_images = selected_images[k_shot:]\n",
    "        \n",
    "        support_set.extend(support_images)\n",
    "        query_set.extend(query_images)\n",
    "        \n",
    "        #* Criar as labels RELATIVAS ao episódio\n",
    "        support_labels.extend([i] * k_shot)\n",
    "        query_labels.extend([i] * k_query)\n",
    "\n",
    "\n",
    "    #* Embaralhar os conjuntos\n",
    "    s_indices = np.random.permutation(len(support_set))\n",
    "    q_indices = np.random.permutation(len(query_set))\n",
    "    \n",
    "    #* Reordenar usando os índices embaralhados\n",
    "    support_set = torch.stack(support_set)[s_indices]\n",
    "    support_labels = torch.tensor(support_labels)[s_indices]\n",
    "    query_set = torch.stack(query_set)[q_indices]\n",
    "    query_labels = torch.tensor(query_labels)[q_indices]\n",
    "    \n",
    "    #*Retorna os 4 tensores para serem usados pelo modelo\n",
    "    return support_set, support_labels, query_set, query_labels\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36a5434",
   "metadata": {},
   "source": [
    "Um tensor é uma estrutura de dados usada para armazenar números em múltiplas dimensões, muito comum em machine learning e deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d28455",
   "metadata": {},
   "source": [
    "### Redes Prototípicas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee50ba6",
   "metadata": {},
   "source": [
    "**A Analogia: O Crítico de Arte Novato**\n",
    "\n",
    "Imagine que seu modelo é um crítico de arte novato. Cada \"episódio\" é um novo desafio para ele.\n",
    "\n",
    "1. O Desafio (Episódio): Levamos o crítico a uma galeria com 5 artistas que ele nunca viu antes (N_WAY = 5).\n",
    "\n",
    "2. O Material de Estudo (support_set): Mostramos a ele apenas uma pintura de cada um desses 5 artistas (K_SHOT = 1). A tarefa dele é estudar essas 5 pinturas e formar uma ideia central do \"estilo\" de cada artista.\n",
    "\n",
    "3. A Ideia Central (Protótipo): Depois de olhar a pintura de um artista, o crítico cria uma \"representação mental\" do estilo daquele artista. Essa representação é o protótipo. Ele faz isso para os 5 artistas.\n",
    "\n",
    "4. O Teste (query_set): Agora, mostramos ao crítico uma nova pintura (que ele não usou para estudo) e perguntamos: \"Qual dos 5 artistas pintou esta obra?\".\n",
    "\n",
    "5. A Resposta (Classificação): O crítico compara a nova pintura com as 5 \"representações mentais\" (protótipos) que ele criou. A que for mais parecida, ele supõe que seja do mesmo artista.\n",
    "\n",
    "6. O Aprendizado (Loss & Optimizer): Se ele errar, nós o corrigimos. Essa correção o ajuda a refinar sua capacidade de extrair a \"essência\" de uma pintura, para que da próxima vez ele forme representações mentais melhores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74d2030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "#* Transforma uma imagem em um vetor de características\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, embedding_dim=64):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        self.convnet = nn.Sequential(\n",
    "            #* Primeiro bloco convolucional\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),  # Aplica uma convolução 2D com 1 canal de entrada, 64 canais de saída, kernel 3x3 e padding de 1.\n",
    "            nn.BatchNorm2d(64),                          # Normaliza a saída da convolução para estabilizar o treinamento.\n",
    "            nn.ReLU(),                                   # Aplica a função de ativação ReLU para introduzir não-linearidade.\n",
    "            nn.MaxPool2d(2),                             # Reduz as dimensões espaciais aplicando pooling máximo 2x2.\n",
    "\n",
    "            #* Segundo bloco convolucional\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), # Outra convolução 2D com 64 canais de entrada e saída, kernel 3x3 e padding de 1.\n",
    "            nn.BatchNorm2d(64),                          # Normaliza a saída da segunda convolução.\n",
    "            nn.ReLU(),                                   # Aplica novamente a função de ativação ReLU.\n",
    "            nn.MaxPool2d(2),                             # Reduz ainda mais as dimensões espaciais com outro pooling máximo 2x2.\n",
    "\n",
    "            #* Terceiro bloco convolucional\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), # Terceira convolução 2D com a mesma configuração.\n",
    "            nn.BatchNorm2d(64),                          # Normaliza a saída da terceira convolução.\n",
    "            nn.ReLU(),                                   # Aplica mais uma vez a função de ativação ReLU.\n",
    "            nn.MaxPool2d(2)                              # Pooling máximo final 2x2 para reduzir ainda mais as dimensões espaciais.\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),  # Transforma a saída 3D (batch_size, canais, altura, largura) em uma saída 2D (batch_size, canais * altura * largura).\n",
    "            nn.Linear(64 * 3 * 3, embedding_dim)  # Aplica uma camada totalmente conectada (fully connected) que reduz a dimensão de entrada (64 * 3 * 3) para o tamanho do embedding especificado (embedding_dim).\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convnet(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf784fc",
   "metadata": {},
   "source": [
    "- \"Convolução 2D\": É uma operação matemática aplicada a imagens (que são matrizes 2D de pixels) para extrair características, como bordas ou texturas.\n",
    "- \"1 canal de entrada\": A imagem de entrada tem 1 canal, ou seja, é uma imagem em preto e branco (escala de cinza).\n",
    "- \"64 canais de saída\": A camada gera 64 imagens de saída (chamadas de mapas de ativação ou feature maps), cada uma destacando diferentes padrões encontrados na imagem.\n",
    "- \"kernel 3x3\": O filtro (ou janela) que percorre a imagem tem tamanho 3x3 pixels. Ele examina pequenos blocos da imagem de cada vez.\n",
    "- \"padding de 1\": Adiciona uma borda de 1 pixel ao redor da imagem para garantir que a saída tenha o mesmo tamanho que a entrada (ou quase)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe633f6",
   "metadata": {},
   "source": [
    "A rede é composta por três blocos de camadas convolucionais (Conv2d), cada um seguido por normalização em lote (BatchNorm2d), função de ativação ReLU e uma camada de pooling (MaxPool2d). O objetivo dessa sequência é extrair representações compactas e discriminativas das imagens, reduzindo gradualmente sua dimensão espacial e aumentando a profundidade das características aprendidas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c4329f",
   "metadata": {},
   "source": [
    "### Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3d5ec9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.46M/9.46M [00:01<00:00, 4.83MB/s]\n"
     ]
    }
   ],
   "source": [
    "#* Preparar dados de treino\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "omniglot_train = tv.datasets.Omniglot(\n",
    "    root=\"../data\",\n",
    "    background=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb8b077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Agrupar imagens por classe, facilitando a amostragem\n",
    "\n",
    "train_class_map = {}\n",
    "for img, label in omniglot_train:\n",
    "    if label not in train_class_map:\n",
    "        train_class_map[label] = []\n",
    "    train_class_map[label].append(img)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6255c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Configurações de treinamento\n",
    "N_WAY = 5\n",
    "K_SHOT = 1\n",
    "K_QUERY = 15\n",
    "NUM_EPS = 1000\n",
    "EMBEDDING_DIM = 64\n",
    "\n",
    "\n",
    "#* Inicializando modelo e otimizador\n",
    "model = EmbeddingNet(embedding_dim=EMBEDDING_DIM)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34969486",
   "metadata": {},
   "source": [
    "- **embedding_dim:** é a quantidade de números (dimensão) que representa cada imagem no \"espaço de características\" aprendido pela rede.\n",
    "<br>Quanto maior o embedding_dim, mais informações a rede pode guardar sobre cada imagem, mas também aumenta a complexidade do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cf8f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:22<00:00, 12.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train is over\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#* Loop de treinamento\n",
    "print(\"Training...\")\n",
    "\n",
    "for episode in tqdm(range(NUM_EPS)):\n",
    "    #* Pegando um novo episódio \n",
    "    support_x, support_y, query_x, query_y  = create_episode(train_class_map, N_WAY, K_SHOT, K_QUERY)\n",
    "    \n",
    "    #* Zerar gradientes do otimizador\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #* Passar imagens de suporte e de query (teste) \n",
    "    support_embeddings = model(support_x) # Gera as representações \"mentais\"\n",
    "    query_embeddings = model(query_x)\n",
    "    \n",
    "    #* Calcular o protótipo de cada classe\n",
    "    prototypes = []\n",
    "    for i in range(N_WAY):  # Itera sobre cada classe na tarefa N-way\n",
    "        class_embeddings = support_embeddings[support_y == i]  # Seleciona as embeddings correspondentes à classe atual\n",
    "        prototypes.append(class_embeddings.mean(dim=0))  # Calcula a média das embeddings da classe e adiciona à lista de protótipos\n",
    "    prototypes = torch.stack(prototypes)  # Empilha todos os protótipos de classe em um único tensor\n",
    "    \n",
    "    #* Calcular a distancia de cada imagem de teste para cada prorótipo\n",
    "    distances = torch.cdist(query_embeddings, prototypes, p=2).pow(2) # Euclideana ao quadrado\n",
    "    \n",
    "    #* Calcular a perda do modelo\n",
    "    log_p_y = F.log_softmax(-distances, dim=1)\n",
    "    loss = F.nll_loss(log_p_y, query_y)\n",
    "    \n",
    "    #*Realiza a correção\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Train is over\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a340f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"prototypical_net.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44699567",
   "metadata": {},
   "source": [
    "### Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb7a7b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Carregando o conjunto de teste\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "omniglot_test = tv.datasets.Omniglot(\n",
    "    root=\"../data\",\n",
    "    background=False, \n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_class_map = {}\n",
    "for img, label in omniglot_test:\n",
    "    if label not in test_class_map:\n",
    "        test_class_map[label] = []\n",
    "    test_class_map[label].append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ceb99c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingNet(\n",
       "  (convnet): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=576, out_features=64, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#* Avaliando o modelo\n",
    "\n",
    "model.load_state_dict(torch.load(\"prototypical_net.pth\", weights_only=False))\n",
    "model.eval()\n",
    "\n",
    "# Essa linha carrega os pesos (parâmetros treinados) do modelo salvos anteriormente no arquivo \n",
    "# \"prototypical_net.pth\" para dentro do modelo atual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97379d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:36<00:00, 27.26it/s]\n"
     ]
    }
   ],
   "source": [
    "NUM_EPS_TESTE = 1000\n",
    "accuracies = []\n",
    "\n",
    "print(\"Testing model...\")\n",
    "with torch.no_grad(): #* Não se preocupa com gradientes, e sim apenas em testar\n",
    "    for episode in tqdm(range(NUM_EPS_TESTE)):\n",
    "        support_x, support_y, query_x, query_y = create_episode(test_class_map, N_WAY, K_SHOT, K_QUERY)\n",
    "        \n",
    "        support_embeddings = model(support_x)\n",
    "        query_embeddings = model(query_x)\n",
    "        \n",
    "        prototypes = []\n",
    "        for i in range(N_WAY):\n",
    "            class_embeddings = support_embeddings[support_y == i]\n",
    "            prototypes.append(class_embeddings.mean(dim=0))\n",
    "        prototypes = torch.stack(prototypes)\n",
    "        \n",
    "        distances = torch.cdist(query_embeddings, prototypes, p=2).pow(2)\n",
    "        \n",
    "        #* torch.argmin encontra o índice da menor distância, que é a nossa classe prevista\n",
    "        predictions = torch.argmin(distances, dim=1)\n",
    "        \n",
    "        #* Comparamos as predições com as labels verdadeiras para calcular a acurácia do episódio\n",
    "        #* (predictions == query_y) -> Tensor de True/False\n",
    "        #* .float() -> Converte para 1.0/0.0\n",
    "        #* .mean() -> Calcula a média (ex: 60 acertos em 75 -> 0.80)\n",
    "        #* .item() -> Extrai o valor numérico do tensor\n",
    "        accuracy = (predictions == query_y).float().mean().item()\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "print(\"Test is over\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6b55a8",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd352803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Resultados da Avaliação ---\n",
      "Acurácia Média: 90.96%\n",
      "Intervalo de Confiança (95%): ± 0.45%\n",
      "Resultado Final: 90.96% ± 0.45%\n"
     ]
    }
   ],
   "source": [
    "mean_accuracy = np.mean(accuracies)\n",
    "std_accuracy = np.std(accuracies)\n",
    "confidence_interval = 1.96 * std_accuracy / np.sqrt(NUM_EPS_TESTE) # Fórmula do IC 95%\n",
    "\n",
    "print(\"\\n--- Resultados da Avaliação ---\")\n",
    "print(f\"Acurácia Média: {mean_accuracy * 100:.2f}%\")\n",
    "print(f\"Intervalo de Confiança (95%): ± {confidence_interval * 100:.2f}%\")\n",
    "print(f\"Resultado Final: {mean_accuracy * 100:.2f}% ± {confidence_interval * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
