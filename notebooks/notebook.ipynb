{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a544d8d",
   "metadata": {},
   "source": [
    "## Few-Shot-Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db17338",
   "metadata": {},
   "source": [
    "### Divisão dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b8e149",
   "metadata": {},
   "source": [
    "Cria um dicionário dos dados (class map): {label_classe: [lista de  imagens dessa classe]} e divide os dados entre treino e teste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5d2955",
   "metadata": {},
   "source": [
    "**Estrutura do Omniglot**\n",
    "\n",
    "\n",
    "omniglot-py/ <br>\n",
    "└── images_background/  (ou images_evaluation/)<br>\n",
    "    ├── Alphabet_of_the_Magi/<br>\n",
    "    │   ├── character01/<br>\n",
    "    │   │   ├── 0709_01.png<br>\n",
    "    │   │   ├── 0709_02.png<br>\n",
    "    │   │   └── ... (20 imagens no total)<br>\n",
    "    │   └── character02/<br>\n",
    "    │       └── ...<br>\n",
    "    ├── Angelic/<br>\n",
    "    │   └── ...<br>\n",
    "    └── ... (mais 30 alfabetos)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55723d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision as tv\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd910afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "omniglot_train = tv.datasets.Omniglot(\n",
    "    root=\"../data\",\n",
    "    background=True, #Conjunto de treino\n",
    "    download=True,\n",
    "    transform=transform \n",
    ")\n",
    "\n",
    "\n",
    "omniglot_test = tv.datasets.Omniglot(\n",
    "    root=\"../data\",\n",
    "    background=False, #Conjunto de teste\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71756d4c",
   "metadata": {},
   "source": [
    "O Omniglot é dividido em background e evaluation. O conjunto background contém os alfabetos para treinar o seu modelo a \"aprender a aprender\". O conjunto evaluation contém alfabetos completamente novos, que o modelo nunca viu, e é usado para testar se ele consegue generalizar para novas classes com poucos exemplos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3d70bd",
   "metadata": {},
   "source": [
    "### Episódios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6691654f",
   "metadata": {},
   "source": [
    "Em vez de treinar o modelo para ser um especialista em um conjunto fixo de classes, nós o treinamos para ser um generalista em aprender.\n",
    "\n",
    "Seria dar ao modelo uma série de \"mini testes surpresa\" (os episódios). Cada teste é uma pequena tarefa de classificação."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266417ca",
   "metadata": {},
   "source": [
    "**Anatomia de um Episódio**\n",
    "\n",
    "Em um cenário N-way K-shot (onde N = 5 e K = 1):\n",
    "\n",
    "1. Amostragem: Primeiro, escolhemos aleatoriamente N classes do nosso enorme conjunto de dados de treinamento. \n",
    "2. Support Set: Para cada uma dessas N classes, damos ao modelo K exemplos. Este é o \"material de estudo\" para este teste específico. O modelo deve olhar para essas \"N times K\" imagens e aprender a distinguir as N classes.\n",
    "3. Query Set: Em seguida, pegamos outras imagens (que não estavam no material de estudo) daquelas mesmas N classes. Estas são as \"perguntas do teste\".\n",
    "4. Avaliação e Aprendizado: O modelo tenta classificar as imagens do Query Set. Calculamos o quão bem ele se saiu (loss). Usamos essa nota para ajustar os pesos do modelo através do backpropagation.\n",
    "\n",
    "\n",
    "Ao repetir esse processo milhares de vezes, com milhares de combinações diferentes de classes, o modelo não está memorizando \"o que é o caractere A do alfabeto Grego\". Em vez disso, ele está aprendendo uma estratégia para, dado qualquer conjunto de N novas classes com K exemplos cada, descobrir as características que as diferenciam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85edbc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_episode(classes, n_way, k_shot, k_query):\n",
    "    episode_classes_idx = random.sample(list(classes.keys()), n_way)\n",
    "    \n",
    "    support_set = []\n",
    "    support_labels = []\n",
    "    query_set = []\n",
    "    query_labels = []\n",
    "    \n",
    "    \n",
    "    #* Montar o Support e o Query set para cada classe selecionada\n",
    "    for i, class_idx in enumerate(episode_classes_idx):\n",
    "        images_of_class = classes[class_idx] \n",
    "        \n",
    "        #* Sortear imagens dessa classe\n",
    "        selected_images = random.sample(images_of_class, k_shot + k_query)\n",
    "        \n",
    "        #* Dividir as imagens sorteadas nos dois conjuntos\n",
    "        support_images = selected_images[:k_shot]\n",
    "        query_images = selected_images[k_shot:]\n",
    "        \n",
    "        support_set.extend(support_images)\n",
    "        query_set.extend(query_images)\n",
    "        \n",
    "        #* Criar as labels RELATIVAS ao episódio\n",
    "        support_labels.extend([i] * k_shot)\n",
    "        query_labels.extend([i] * k_query)\n",
    "\n",
    "\n",
    "    #* Embaralhar os conjuntos\n",
    "    s_indices = np.random.permutation(len(support_set))\n",
    "    q_indices = np.random.permutation(len(query_set))\n",
    "    \n",
    "    #* Reordenar usando os índices embaralhados\n",
    "    support_set = torch.stack(support_set)[s_indices]\n",
    "    support_labels = torch.tensor(support_labels)[s_indices]\n",
    "    query_set = torch.stack(query_set)[q_indices]\n",
    "    query_labels = torch.tensor(query_labels)[q_indices]\n",
    "    \n",
    "    #*Retorna os 4 tensores para serem usados pelo modelo\n",
    "    return support_set, support_labels, query_set, query_labels\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d28455",
   "metadata": {},
   "source": [
    "### Redes Prototípicas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee50ba6",
   "metadata": {},
   "source": [
    "A Analogia: O Crítico de Arte Novato\n",
    "\n",
    "Imagine que seu modelo é um crítico de arte novato. Cada \"episódio\" é um novo desafio para ele.\n",
    "\n",
    "O Desafio (Episódio): Levamos o crítico a uma galeria com 5 artistas que ele nunca viu antes (N_WAY = 5).\n",
    "\n",
    "O Material de Estudo (support_set): Mostramos a ele apenas uma pintura de cada um desses 5 artistas (K_SHOT = 1). A tarefa dele é estudar essas 5 pinturas e formar uma ideia central do \"estilo\" de cada artista.\n",
    "\n",
    "A Ideia Central (Protótipo): Depois de olhar a pintura de um artista, o crítico cria uma \"representação mental\" do estilo daquele artista. Essa representação é o protótipo. Ele faz isso para os 5 artistas.\n",
    "\n",
    "O Teste (query_set): Agora, mostramos ao crítico uma nova pintura (que ele não usou para estudo) e perguntamos: \"Qual dos 5 artistas pintou esta obra?\".\n",
    "\n",
    "A Resposta (Classificação): O crítico compara a nova pintura com as 5 \"representações mentais\" (protótipos) que ele criou. A que for mais parecida, ele supõe que seja do mesmo artista.\n",
    "\n",
    "O Aprendizado (Loss & Optimizer): Se ele errar, nós o corrigimos. Essa correção o ajuda a refinar sua capacidade de extrair a \"essência\" de uma pintura, para que da próxima vez ele forme representações mentais melhores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e74d2030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "#* Transforma uma imagem em um vetor de características\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, embedding_dim=64):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        self.convnet = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 3 * 3, embedding_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convnet(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe633f6",
   "metadata": {},
   "source": [
    "A rede é composta por três blocos de camadas convolucionais (Conv2d), cada um seguido por normalização em lote (BatchNorm2d), função de ativação ReLU e uma camada de pooling (MaxPool2d). O objetivo dessa sequência é extrair representações compactas e discriminativas das imagens, reduzindo gradualmente sua dimensão espacial e aumentando a profundidade das características aprendidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d5ec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Preparar dados de treino\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "omniglot_train = tv.datasets.Omniglot(\n",
    "    root=\"../data\",\n",
    "    background=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8b077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Agrupar imagens por classe, facilitando a amostragem\n",
    "\n",
    "train_class_map = {}\n",
    "for img, label in omniglot_train:\n",
    "    if label not in train_class_map:\n",
    "        train_class_map[label] = []\n",
    "    train_class_map[label].append(img)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6255c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Configurações de treinamento\n",
    "N_WAY = 5\n",
    "K_SHOT = 1\n",
    "K_QUERY = 15\n",
    "NUM_EPS = 1000\n",
    "EMBEDDING_DIM = 64\n",
    "\n",
    "\n",
    "#* Inicializando modelo e otimizador\n",
    "model = EmbeddingNet(embedding_dim=EMBEDDING_DIM)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cf8f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:19<00:00, 12.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conclued\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#* Loop de treinamento\n",
    "print(\"Training...\")\n",
    "\n",
    "for episode in tqdm(range(NUM_EPS)):\n",
    "    #* Pegando um novo episódio \n",
    "    support_x, support_y, query_x, query_y  = create_episode(train_class_map, N_WAY, K_SHOT, K_QUERY)\n",
    "    \n",
    "    #* Zerar gradientes do otimizador\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #* Passar imagens de suporte e de query (teste) \n",
    "    support_embeddings = model(support_x) # Gera as representações \"mentais\"\n",
    "    query_embeddings = model(query_x)\n",
    "    \n",
    "    #* Calcular o protótipo de cada classe\n",
    "    prototypes = []\n",
    "    for i in range(N_WAY):\n",
    "        class_embeddings = support_embeddings[support_y == i]\n",
    "        prototypes.append(class_embeddings.mean(dim=0))\n",
    "    prototypes = torch.stack(prototypes)\n",
    "    \n",
    "    #* Calcular a distancia de cada imagem de teste para cada prorótipo\n",
    "    distances = torch.cdist(query_embeddings, prototypes, p=2).pow(2) # Euclideana ao quadrado\n",
    "    \n",
    "    #* Calcular a perda do modelo\n",
    "    log_p_y = F.log_softmax(-distances, dim=1)\n",
    "    loss = F.nll_loss(log_p_y, query_y)\n",
    "    \n",
    "    #*Realiza a correção\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Train is over\")\n",
    "\n",
    "torch.save(model.state_dict, \"prototypical_net.pth\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7a7b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Carregando o conjunto de teste\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "omniglot_test = tv.datasets.Omniglot(\n",
    "    root=\"../data\",\n",
    "    background=False, \n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_class_map = {}\n",
    "for img, label in omniglot_test:\n",
    "    if label not in test_class_map:\n",
    "        test_class_map[label] = []\n",
    "    test_class_map[label].append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ceb99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Avaliando o modelo\n",
    "\n",
    "model.load_state_dict(torch.load(\"prototypical_net.pth\"))\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
